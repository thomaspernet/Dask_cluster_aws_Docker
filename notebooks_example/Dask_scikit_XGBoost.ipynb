{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit learn and XGBoost with Dask\n",
    "\n",
    "In this notebook, we will need a dataset available in my S3. it's a public folder, so feel free to download the data:\n",
    "\n",
    "- X_train: https://tutorial-machine-learning.s3.eu-west-3.amazonaws.com/Dask-dataset/X_train.csv\n",
    "- Y_train: https://tutorial-machine-learning.s3.eu-west-3.amazonaws.com/Dask-dataset/y_train.csv\n",
    "\n",
    "The dataset contain 53630 observations each. The train set has 40 features, among which V_0, V_1 and V_2 are object. \n",
    "\n",
    "In this notebook, I only aim at showing you how to run a random forest and an XGBoost on a fargate cluster using Dask and scikit learn/XGboost. I don't intend to create the best ML model. Besides, the dataset is too small to learn something useful since the target is highly imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the cluster\n",
    "\n",
    "We need to set the cluster with `dask_cloudprovider` librairy. It can take few minutes to run. If the number of cluster is not enough, you can increase it with `cluster.scale(4)`. \n",
    "\n",
    "If you want to run on a local cluster, you can use this command\n",
    "\n",
    "```\n",
    "client = Client(processes=False,\n",
    "                threads_per_worker=4,\n",
    "                n_workers=1)  # set up local cluster on your laptop\n",
    "```\n",
    "\n",
    "When the cluster is open you can monitor it with the provided URL. For instance: `http://3.9.190.245:8787/status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/envs/daskenv/lib/python3.8/contextlib.py:120: UserWarning: Creating your cluster is taking a surprisingly long time. This is likely due to pending resources on AWS. Hang tight! \n",
      "  next(self.gen)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://3.9.190.245:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://3.9.190.245:8787/status' target='_blank'>http://3.9.190.245:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>64.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.31.12.42:8786' processes=4 threads=16, memory=64.00 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_cloudprovider import FargateCluster\n",
    "cluster = FargateCluster(n_workers=4,\n",
    "                         image='thomaspernet/dask-container:py-38'\n",
    "                        )\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### X train \n",
    "p_x = \"https://tutorial-machine-learning.s3.eu-west-3.amazonaws.com/\" \\\n",
    "\"Dask-dataset/X_train.csv\"\n",
    "X_train= (\n",
    "    dd.read_csv(p_x,\n",
    "                low_memory=False,\n",
    "               dtype={'V_0': 'category', ### Important !\n",
    "                      'V_0':'category', ### Important !\n",
    "                      'V_0':'category' ### Important !\n",
    "                     }\n",
    "               )   \n",
    "    .categorize()\n",
    ")\n",
    "\n",
    "#### Y train\n",
    "p_y = \"https://tutorial-machine-learning.s3.eu-west-3.amazonaws.com/\" \\\n",
    "\"Dask-dataset/y_train.csv\"\n",
    "y_train= (\n",
    "    dd.read_csv(p_y,\n",
    "                low_memory=False,\n",
    "                #usecols = features,\n",
    "               )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pipeline line\n",
    "\n",
    "Our pipeline is simple:\n",
    "\n",
    "1. Create robust standardise value-> Large outliers so better to use robust\n",
    "2. Convert category to one hot encoder. Note that, to make it works, we must load the object features as `category`\n",
    "3. Build the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/envs/daskenv/lib/python3.8/site-packages/dask_ml/model_selection/_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train,\n",
    "                                                    y_train,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we are using Dask DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V_0</th>\n",
       "      <th>V_1</th>\n",
       "      <th>V_2</th>\n",
       "      <th>V_3</th>\n",
       "      <th>V_4</th>\n",
       "      <th>V_5</th>\n",
       "      <th>V_6</th>\n",
       "      <th>V_7</th>\n",
       "      <th>V_8</th>\n",
       "      <th>V_9</th>\n",
       "      <th>V_10</th>\n",
       "      <th>V_11</th>\n",
       "      <th>V_12</th>\n",
       "      <th>V_13</th>\n",
       "      <th>V_14</th>\n",
       "      <th>V_15</th>\n",
       "      <th>V_16</th>\n",
       "      <th>V_17</th>\n",
       "      <th>V_18</th>\n",
       "      <th>V_19</th>\n",
       "      <th>V_20</th>\n",
       "      <th>V_21</th>\n",
       "      <th>V_22</th>\n",
       "      <th>V_23</th>\n",
       "      <th>V_24</th>\n",
       "      <th>V_25</th>\n",
       "      <th>V_26</th>\n",
       "      <th>V_27</th>\n",
       "      <th>V_28</th>\n",
       "      <th>V_29</th>\n",
       "      <th>V_30</th>\n",
       "      <th>V_31</th>\n",
       "      <th>V_32</th>\n",
       "      <th>V_33</th>\n",
       "      <th>V_34</th>\n",
       "      <th>V_35</th>\n",
       "      <th>V_36</th>\n",
       "      <th>V_37</th>\n",
       "      <th>V_38</th>\n",
       "      <th>V_39</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>category[known]</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: split, 6 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                           V_0              V_1              V_2      V_3      V_4      V_5      V_6      V_7      V_8      V_9     V_10     V_11     V_12     V_13     V_14     V_15     V_16     V_17     V_18     V_19     V_20     V_21     V_22     V_23     V_24     V_25     V_26     V_27     V_28     V_29     V_30     V_31     V_32     V_33     V_34     V_35     V_36     V_37     V_38     V_39\n",
       "npartitions=1                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "               category[known]  category[known]  category[known]  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64\n",
       "                           ...              ...              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "Dask Name: split, 6 tasks"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test set has 105 label equals to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5329\n",
       "1     105\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.compute()['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from dask_ml.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_obj = (X_train\n",
    "            .dtypes\n",
    "            .loc[lambda x : \n",
    "                 (x =='category') \n",
    "                &(x.index != 'status')]\n",
    "            .index\n",
    "           )\n",
    "feat_cont = (X_train\n",
    "            .dtypes\n",
    "            .loc[lambda x : x =='float64']\n",
    "            .index\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = make_pipeline(\n",
    "    RobustScaler()\n",
    "    #StandardScaler()\n",
    ")\n",
    "cat_proc = make_pipeline(\n",
    "    OneHotEncoder()\n",
    ")\n",
    "preprocessor = make_column_transformer(\n",
    "    (num_proc, tuple(feat_cont)),\n",
    "    (cat_proc, tuple(feat_obj))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Random forest\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "- Step 1\n",
    "  - Call pipeline\n",
    "- Step 2\n",
    "  - Stratified K-fold\n",
    "- Step 3\n",
    "  - Evaluate the model\n",
    "  \n",
    "We'll fit a [random forest classfier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), using [grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to find the best value of the $n$ number of estimators hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(preprocessor,\n",
    "                    RandomForestClassifier(random_state=0))\n",
    "param_grid = {\n",
    "    \"randomforestclassifier__n_estimators\": [300, 400],\n",
    "}\n",
    "# Create grid search -> Use \n",
    "random_search = RandomizedSearchCV(clf,\n",
    "                                   param_grid,\n",
    "                                   cv=cv,\n",
    "                                   scoring = 'recall') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn uses joblib for single-machine parallelism. This lets you train most estimators (anything that accepts an n_jobs parameter) using all the cores of your laptop or workstation.\n",
    "\n",
    "Alternatively, Scikit-Learn can use Dask for parallelism. This lets you train those estimators using all the cores of your cluster without significantly changing your code.\n",
    "\n",
    "This is most useful for training large models on medium-sized datasets. You may have a large model when searching over many hyper-parameters, or when using an ensemble method with many individual estimators. For too small datasets, training times will typically be small enough that cluster-wide parallelism isn't helpful. For too large datasets (larger than a single machine's memory), the scikit-learn estimators may not be able to cope (see below).\n",
    "\n",
    "We fit 2 different models, with 30 repeatitions each, one for each hyper-parameter combination in param_grid, distributed across the cluster. At this point, we have a regular scikit-learn model, which can be used for prediction, scoring, etc.\n",
    "\n",
    "![](https://github.com/thomaspernet/Dask_cluster_aws_Docker/blob/master/notebooks_example/images/random_forest.gif?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/envs/daskenv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:278: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/thomas/anaconda3/envs/daskenv/lib/python3.8/site-packages/sklearn/utils/validation.py:68: FutureWarning: Pass classifier=True as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 890 ms, sys: 238 ms, total: 1.13 s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    randomtree = random_search.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 3 minutes and 49 seconds to train and the model managed to find 24 over the 101 label 1, or a recall of merely 20%.\n",
    "\n",
    "If you want to compute the confusion matrix, or make the prediction, it's important to add `.compute()` before so that we ask Dask to perform the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': 'randomforestclassifier',\n",
       " 'best_params': {'randomforestclassifier__n_estimators': 300},\n",
       " 'score': [{'metric': 'recall',\n",
       "   'mean_test_score': [0.19306603094254246, 0.18895009522742978],\n",
       "   'std_test_score': [0.04715511718050672, 0.04477014534307923],\n",
       "   'confusion_matrix': {1: 5325, 2: 4, 3: 81, 4: 24},\n",
       "   'classification_report': {'Not User': {'precision': 0.9850166481687015,\n",
       "     'recall': 0.9992493901294802,\n",
       "     'f1-score': 0.992081974848626,\n",
       "     'support': 5329},\n",
       "    'User': {'precision': 0.8571428571428571,\n",
       "     'recall': 0.22857142857142856,\n",
       "     'f1-score': 0.3609022556390978,\n",
       "     'support': 105},\n",
       "    'accuracy': 0.9843577475156422,\n",
       "    'macro avg': {'precision': 0.9210797526557792,\n",
       "     'recall': 0.6139104093504544,\n",
       "     'f1-score': 0.6764921152438619,\n",
       "     'support': 5434},\n",
       "    'weighted avg': {'precision': 0.9825457707197295,\n",
       "     'recall': 0.9843577475156422,\n",
       "     'f1-score': 0.9798858264281253,\n",
       "     'support': 5434}}}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat =confusion_matrix(y_test, randomtree.predict(X_test.compute()))    \n",
    "    \n",
    "dic_metrics = {\n",
    "    \"classifier\": list(randomtree.best_estimator_.named_steps.keys())[-1],\n",
    "    \"best_params\": randomtree.best_params_,\n",
    "    \"score\": [\n",
    "        {\n",
    "            \"metric\": 'recall',\n",
    "            'mean_test_score':list(randomtree.cv_results_['mean_test_score']),\n",
    "            'std_test_score':list(randomtree.cv_results_['std_test_score']),\n",
    "            \"confusion_matrix\": dict(enumerate(conf_mat.flatten(), 1)),\n",
    "            \"classification_report\": classification_report(\n",
    "                y_test,\n",
    "                randomtree.predict(X_test.compute()),\n",
    "                target_names=[\"Not User\", \"User\"],\n",
    "                output_dict=True,\n",
    "            ),\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "dic_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: XGboost\n",
    "\n",
    "We'll fit a [XGBoost classfier](https://xgboost.readthedocs.io/en/latest/python/python_api.html), using [grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to find the best value of the $n$ number of estimators hyperparameter.\n",
    "\n",
    "Note that, I didn't manage to make `dask_ml.xgboost` works on the cluster, so I rely on the official librairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \n",
    " 'xgbclassifier__n_estimators':[100, 250],\n",
    "}\n",
    "\n",
    "random_search = GridSearchCV(clf, param_grid, cv=cv, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(\n",
    "    preprocessor,\n",
    "    XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=13,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic')\n",
    ")\n",
    "random_search = GridSearchCV(clf, param_grid, cv=cv, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/thomaspernet/Dask_cluster_aws_Docker/blob/master/notebooks_example/images/XGboost.gif?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/envs/daskenv/lib/python3.8/site-packages/sklearn/utils/validation.py:68: FutureWarning: Pass classifier=True as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 505 ms, sys: 77.5 ms, total: 582 ms\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    xgboost_bm = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 2 minutes and 19 seconds to train and the model managed to find 46 over the 101 label 1, or a recall of merely 43%, but overfit since the recall on the train is about 33% on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': 'xgbclassifier',\n",
       " 'best_params': {'xgbclassifier__n_estimators': 250},\n",
       " 'score': [{'metric': 'recall',\n",
       "   'mean_test_score': [0.3292594114337853, 0.35111853639828944],\n",
       "   'std_test_score': [0.04151084154626891, 0.0402538824948912],\n",
       "   'confusion_matrix': {1: 5307, 2: 22, 3: 59, 4: 46},\n",
       "   'classification_report': {'Not User': {'precision': 0.9890048453224003,\n",
       "     'recall': 0.9958716457121412,\n",
       "     'f1-score': 0.9924263674614306,\n",
       "     'support': 5329},\n",
       "    'User': {'precision': 0.6764705882352942,\n",
       "     'recall': 0.4380952380952381,\n",
       "     'f1-score': 0.5317919075144508,\n",
       "     'support': 105},\n",
       "    'accuracy': 0.9850938535149062,\n",
       "    'macro avg': {'precision': 0.8327377167788472,\n",
       "     'recall': 0.7169834419036896,\n",
       "     'f1-score': 0.7621091374879407,\n",
       "     'support': 5434},\n",
       "    'weighted avg': {'precision': 0.9829658138549462,\n",
       "     'recall': 0.9850938535149062,\n",
       "     'f1-score': 0.983525627988771,\n",
       "     'support': 5434}}}]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat =confusion_matrix(y_test, xgboost_bm.predict(X_test.compute()))    \n",
    "    \n",
    "dic_metrics = {\n",
    "    \"classifier\": list(xgboost_bm.best_estimator_.named_steps.keys())[-1],\n",
    "    \"best_params\": xgboost_bm.best_params_,\n",
    "    \"score\": [\n",
    "        {\n",
    "            \"metric\": 'recall',\n",
    "            'mean_test_score':list(xgboost_bm.cv_results_['mean_test_score']),\n",
    "            'std_test_score':list(xgboost_bm.cv_results_['std_test_score']),\n",
    "            \"confusion_matrix\": dict(enumerate(conf_mat.flatten(), 1)),\n",
    "            \"classification_report\": classification_report(\n",
    "                y_test,\n",
    "                xgboost_bm.predict(X_test.compute()),\n",
    "                target_names=[\"Not User\", \"User\"],\n",
    "                output_dict=True,\n",
    "            ),\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "dic_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
